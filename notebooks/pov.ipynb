{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599477968386",
   "display_name": "Python 3.6.10 64-bit ('nlp_text_similarity': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank Text Similarity\n",
    "### Techniques: tf-idf + svd\n",
    "### Technos: pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "* Spark 2.4.4 is installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment variables\n",
    "\n",
    "* workload: 1 hour to config (first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup SPARK environment variables\n",
    "import os\n",
    "import sys  \n",
    "os.environ['SPARK_HOME'] = '/usr/local/Cellar/apache-spark/2.4.4/libexec'\n",
    "os.environ['PYSPARK_PYTHON'] = '/Applications/anaconda3/envs/nlp_text_similarity/bin/python'  \n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/Applications/anaconda3/envs/nlp_text_similarity/bin/python'  \n",
    "os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home'\n",
    "os.environ['PYTHONPATH'] = os.environ['SPARK_HOME'] + '/python/lib/'\n",
    "sys.path.insert(0, os.environ['SPARK_HOME'] + '/python/lib/py4j-0.10.7-src.zip')\n",
    "sys.path.insert(0, os.environ['SPARK_HOME'] + '/python/lib/pyspark.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list dependencies\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset path\n",
    "os.chdir('..')\n",
    "project_path = os.getcwd()\n",
    "dataset_path = 'data/02_preprocessed/train.csv'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparkSession\n",
    "sparkSession = SparkSession.builder.appName(\"my_app\").master(\"local[*]\").getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x11ad062b0>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://steeve.home:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.4</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>my_app</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# print spark session config\n",
    "sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   sentiment                                             review\n0          0  Working with one of the best Shakespeare sourc...\n1          0  Well...tremors I, the original started off in ...\n2          0  Ouch! This one was a bit painful to sit throug...\n3          0  I've seen some crappy movies in my life, but t...\n4          0  \"Carriers\" follows the exploits of two guys an...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Working with one of the best Shakespeare sourc...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Well...tremors I, the original started off in ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>Ouch! This one was a bit painful to sit throug...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>I've seen some crappy movies in my life, but t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>\"Carriers\" follows the exploits of two guys an...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "# Load imdb dataset as pandas dataframe (<1 sec)\n",
    "dataset_df = pd.read_csv(dataset_path)\n",
    "dataset_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    Working with one of the best Shakespeare sourc...\n1    Well...tremors I, the original started off in ...\n2    Ouch! This one was a bit painful to sit throug...\n3    I've seen some crappy movies in my life, but t...\n4    \"Carriers\" follows the exploits of two guys an...\nName: review, dtype: object"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "# filter dataset (keep reviews)\n",
    "reviews_dataset_df = dataset_df['review']\n",
    "reviews_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+--------------------+\n|               value|\n+--------------------+\n|Working with one ...|\n|Well...tremors I,...|\n|Ouch! This one wa...|\n|I've seen some cr...|\n|\"Carriers\" follow...|\n|I had been lookin...|\n|Effect(s) without...|\n|This picture star...|\n|I chose to see th...|\n|This film has to ...|\n|I felt brain dead...|\n|A young scientist...|\n|Inept, boring, an...|\n|From the first ti...|\n|I find it hard to...|\n|I actually saw Ch...|\n|I went to school ...|\n|I haven't seen th...|\n|I haven't seen an...|\n|One would think t...|\n+--------------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "# create spark dataframe (2 sec)\n",
    "dataset_spark = sparkSession.createDataFrame(data=reviews_dataset_df, schema=StringType())\n",
    "dataset_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(numFeatures=10000)\n",
    "tf = hashingTF.transform(dataset_spark)\n",
    "\n",
    "# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "# First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "# spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "# which occur in less than a minimum number of documents.\n",
    "# In such cases, the IDF for these terms is set to 0.\n",
    "# This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "tfidfIgnore = idfIgnore.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## References\n",
    "\n",
    "https://towardsdatascience.com/multi-class-text-classification-with-pyspark-7d78d022ed35  "
   ]
  }
 ]
}