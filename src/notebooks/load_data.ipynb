{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599404327742",
   "display_name": "Python 3.6.10 64-bit ('nlp_text_similarity': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The project path is: /Users/steeve_laquitaine/Desktop/CodeHub/nlp_txt_similarity\n"
    }
   ],
   "source": [
    "# set data paths  \n",
    "os.chdir('..')\n",
    "print('The project path is:', os.getcwd())\n",
    "\n",
    "data_path           = 'data/'\n",
    "raw_data_path       = data_path + \"01_raw/\"\n",
    "init_dataset_path   = data_path + \"02_intermediate/\"\n",
    "url                 = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz' \n",
    "download_output     = raw_data_path + \"/dataset_compressed.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'wget' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3ff9859f4e46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'wget' is not defined"
     ]
    }
   ],
   "source": [
    "# download\n",
    "wget.download(url, download_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_dataset(data_file, output_folder):\n",
    "    # decompress dataset file in dataset/ folder\n",
    "    tic = time.time()\n",
    "    tar = tarfile.open(data_file)\n",
    "    tar.extractall(path=output_folder)\n",
    "    tar.close()\n",
    "    toc = time.time()\n",
    "    print(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompress_dataset(download_output, raw_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(path, CLASSES):\n",
    "    texts, labels = [], []\n",
    "    for idx, label in enumerate(CLASSES):\n",
    "        for fname in (path / label).glob('*.*'):\n",
    "            texts.append(fname.open('r', encoding='utf-8').read())\n",
    "            labels.append(idx)\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "def extract_transform_load_dataset(raw_data_path, output_path, timeit=True):\n",
    "  \n",
    "    tic = time.time()\n",
    "    \n",
    "    # 1 - Init variables\n",
    "    BOS = 'xbos'  # beginning-of-sentence tag\n",
    "    FLD = 'xfld'  # data field tag\n",
    "    CLASSES = ['neg', 'pos', 'unsup']\n",
    "    col_names = ['sentiment', 'review']\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # 2 - Build output folders\n",
    "    PATH = Path(raw_data_path + 'aclImdb/')\n",
    "    CLAS_PATH = Path(output_path)\n",
    "\n",
    "    # 3 - Processing and storing train dataset\n",
    "    print(PATH)\n",
    "    trn_texts, trn_labels = get_texts(PATH / 'train', CLASSES)\n",
    "    print(len(trn_texts))\n",
    "    print(len(trn_labels))\n",
    "    #trn_idx = np.random.permutation(len(trn_texts))\n",
    "    #trn_texts = trn_texts[trn_idx]\n",
    "    #trn_labels = trn_labels[trn_idx]\n",
    "    df_trn = pd.DataFrame({'review': trn_texts, 'sentiment': trn_labels}, columns=col_names)\n",
    "    df_trn[df_trn['sentiment'] != 2].to_csv(CLAS_PATH / 'train.csv', index=False)\n",
    "    \n",
    "    # 4 - Processing and storing evaluation dataset\n",
    "    val_texts, val_labels = get_texts(PATH / 'test', CLASSES)\n",
    "    #val_idx = np.random.permutation(len(val_texts))\n",
    "    #val_texts = val_texts[val_idx]\n",
    "    #val_labels = val_labels[val_idx]\n",
    "    df_val = pd.DataFrame({'review': val_texts, 'sentiment': val_labels}, columns=col_names)\n",
    "    df_val.to_csv(CLAS_PATH / 'test.csv', index=False)\n",
    "    \n",
    "    # 5 - Store the classes\n",
    "    (CLAS_PATH / 'classes.txt').open('w', encoding='utf-8').writelines(f'{o}\\n' for o in CLASSES)\n",
    "\n",
    "    toc = time.time()\n",
    "    print(np.round(toc - tic, 2), ' sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_transform_load_dataset(raw_data_path, init_dataset_path, timeit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_path, test_path, sample=5000):\n",
    "    \n",
    "    tic = time.time()\n",
    "\n",
    "    # TRAIN\n",
    "    train_dataset = pd.read_csv(train_path).sample(n=sample)\n",
    "\n",
    "    # preview data\n",
    "    print(train_dataset.head())\n",
    "\n",
    "    # build train and test datasets\n",
    "    train_reviews = np.array(train_dataset['review'])\n",
    "    train_sentiments = np.array(train_dataset['sentiment'])\n",
    "\n",
    "    # TEST\n",
    "    test_dataset = pd.read_csv(test_path).sample(n=sample)\n",
    "    test_reviews = np.array(test_dataset['review'])\n",
    "    test_sentiments = np.array(test_dataset['sentiment'])\n",
    "    print('(load_dataset)', time.time() - tic)\n",
    "\n",
    "    toc = time.time()\n",
    "    print(('Completed'), toc - tic)\n",
    "\n",
    "    return train_sentiments, test_sentiments, train_reviews, test_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = load_dataset(train_path, test_path, sample=5000)\n"
   ]
  }
 ]
}